{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {ch: i+1  for i, ch in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: ch for ch, i, in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the training set of bigrams and targets (x, y)\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    xs.append(stoi[ch1])\n",
    "    ys.append(stoi[ch2])\n",
    "    #print(f'{ch1}, {ch2}')\n",
    "    \n",
    "xsT = torch.tensor(xs)\n",
    "ysT = torch.tensor(ys)\n",
    "\n",
    "xsT, ysT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "xenc = F.one_hot(xsT, num_classes=27).float()\n",
    "\n",
    "# Initialize weights\n",
    "W = torch.randn(27, 27, requires_grad=True)\n",
    "logits = xenc @ W # log-counts\n",
    "counts = logits.exp()  # Equivalent to N frequency matrix\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "# plt.imshow(xenc, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss\n",
    "loss = -probs[torch.arange(len(ysT)), ysT].log().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss.item()=20.107805252075195\n"
     ]
    }
   ],
   "source": [
    "print(f'{loss.item()=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "W.grad = None # zero the gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the weights\n",
    "learning_rate = 8\n",
    "with torch.no_grad():\n",
    "   W -= learning_rate * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(20.1078, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'{loss=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Example word list for testing\n",
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "# Initialize the stoi dictionary (character to index mapping)\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {ch: i+1 for i, ch in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: ch for ch, i, in stoi.items()}\n",
    "\n",
    "# Create the training set of bigrams and targets (x, y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "        #print(f'{ch1}, {ch2}')\n",
    "\n",
    "xsT = torch.tensor(xs)\n",
    "ysT = torch.tensor(ys)\n",
    "\n",
    "# Forward pass\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "xenc = F.one_hot(xsT, num_classes=27).float()\n",
    "\n",
    "# Initialize weights\n",
    "W = torch.randn(27, 27, requires_grad=True)\n",
    "\n",
    "# Define a learning rate\n",
    "learning_rate = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training loop (for illustration, using only 2 iterations)\n",
    "for i in range(400):\n",
    "    # Compute logits\n",
    "    logits = xenc @ W  # it will output a row within W whichis the same a s a ROW in makemore counting bigrams version\n",
    "    \n",
    "    # Compute probabilities\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute the loss\n",
    "    # 0.01 * (W**2).sum() is the regularization term that pushes towards a uniform distribution it is needed to prevent overfittting and smooting the moel\n",
    "    loss = -probs[torch.arange(len(ysT)), ysT].log().mean() + 0.01 * (W**2).mean() \n",
    "    # print(f'Iteration {i+1}, {loss.item()=}')\n",
    "    \n",
    "    # Backward pass\n",
    "    W.grad = None  # zero the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    with torch.no_grad():\n",
    "        W -= learning_rate * W.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0020, 0.1372, 0.0405,  ..., 0.0049, 0.0166, 0.0288],\n",
       "        [0.1940, 0.0327, 0.0066,  ..., 0.0068, 0.0517, 0.0080],\n",
       "        [0.0721, 0.3837, 0.0112,  ..., 0.0049, 0.0360, 0.0039],\n",
       "        ...,\n",
       "        [0.2030, 0.2169, 0.0033,  ..., 0.0017, 0.0059, 0.0050],\n",
       "        [0.0534, 0.3424, 0.0085,  ..., 0.0054, 0.0207, 0.0177],\n",
       "        [0.0848, 0.1330, 0.0243,  ..., 0.0205, 0.0050, 0.0047]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W  # it will output a row within W whichis the same a s a ROW in makemore counting bigrams version\n",
    "    \n",
    "# Compute probabilities\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexbr.\n",
      "momasurailezityha.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ---------------\n",
    "    # BEFORE:\n",
    "    # p = P[ix]\n",
    "    # ---------------\n",
    "    # NOW: \n",
    "    with torch.no_grad():\n",
    "      xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "      logits = xenc @ W # pre3dict the logs-count\n",
    "      counts = logits.exp()\n",
    "      probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "      \n",
    "      ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "      out.append(itos[ix])\n",
    "      if ix == 0:\n",
    "        break\n",
    "  print(''.join(out))\n",
    "      \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
